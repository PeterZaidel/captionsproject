{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/p.zaydel/conda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import pandas\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import copy\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir='/home/p.zaydel/ProjectNeuralNets/coco_dataset/'\n",
    "imagesDirTrain = '{}train2017/train2017'.format(dataDir)\n",
    "imagesDirVal = '{}val2017/val2017'.format(dataDir)\n",
    "\n",
    "annTrainFile = '{}/annotations_trainval2017/annotations/captions_train2017.json'.format(dataDir)\n",
    "annValFile = '{}/annotations_trainval2017/annotations/captions_val2017.json'.format(dataDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_shapes(coco):\n",
    "    shapes = []\n",
    "    imids = coco.getImgIds()\n",
    "    for imid in imids:\n",
    "        img_data = coco.loadImgs([imid])[0]\n",
    "        h = img_data['height']\n",
    "        w = img_data['width']\n",
    "        shapes.append([w,h])\n",
    "    return shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.25s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([577.71206472, 484.09889506])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_shapes = get_image_shapes(COCO(annTrainFile))\n",
    "np.array(train_shapes).mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.06s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([573.7548, 483.543 ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_shapes = get_image_shapes(COCO(annValFile))\n",
    "np.array(test_shapes).mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms.Resize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([  transforms.Resize(),transforms.ToTensor()\n",
    "                                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSCOCODataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, annFile, imagesDir, transform=None):\n",
    "        self.coco = COCO(annFile)\n",
    "        self.imagesDir = imagesDir\n",
    "        self.imageids = self.coco.getImgIds()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.coco.dataset['images'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imid = self.imageids[idx]\n",
    "        \n",
    "        img_data = self.coco.loadImgs([imid])[0]\n",
    "        \n",
    "        annIds = self.coco.getAnnIds(imgIds=imid)\n",
    "        \n",
    "        anns_data = self.coco.loadAnns(annIds)\n",
    "        anns = [ann['caption'] for ann in anns_data]\n",
    "        \n",
    "        \n",
    "        img_file_name = '{}/{}'.format(self.imagesDir, img_data['file_name'])\n",
    "        image = io.imread(img_file_name)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        sample = {'image': image, 'anns': anns}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.03s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "trainDataset = MSCOCODataset(annTrainFile,imagesDirTrain )\n",
    "testDataset = MSCOCODataset(annValFile,imagesDirVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionNet(nn.Module):\n",
    "    def __init__(self, image_size, hidden_size):\n",
    "        super(CaptionNet, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.cnn = models.inception_v3\n",
    "        self.lstm = nn.LSTM()\n",
    "          \n",
    "    def forward(self, X, hidden=None, C=None):\n",
    "        output = Variable(torch.zeros(X.size(0), self.output_size))\n",
    "        if hidden is None:\n",
    "            hidden = Variable(torch.zeros(self.hidden_size), requires_grad=False)\n",
    "        if C is None:\n",
    "            C = Variable(torch.zeros(self.hidden_size), requires_grad=False)\n",
    "        \n",
    "        for i in range(X.size(0)):    \n",
    "            f_i = self.forget(torch.mv(self.w_f_h, hidden) + torch.mv(self.w_f_i, X[i]) + self.b_f_h + self.b_f_i)\n",
    "            input_i = self.forget(torch.mv(self.w_i_h, hidden) + torch.mv(self.w_i_i, X[i]) + self.b_i_i + self.b_i_h)\n",
    "            C = f_i * C + input_i * self.tanh(torch.mv(self.w_c_h, hidden) + torch.mv(self.w_c_i, X[i]) + self.b_c_i + self.b_c_h)\n",
    "            \n",
    "            output[i] = self.forget(torch.mv(self.w_o_h, hidden) + torch.mv(self.w_o_i, X[i]) + self.b_o_i + self.b_o_h)\n",
    "            hidden = output[i].clone() * self.tanh(C)\n",
    "            \n",
    "        return output, hidden, C"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
