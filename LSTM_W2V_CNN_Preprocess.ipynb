{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import pandas\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import copy\n",
    "import shutil\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from dataset import MSCOCODataset\n",
    "from torch.optim import lr_scheduler\n",
    "from autocorrect import spell\n",
    "import nltk\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1\n",
    "b =1\n",
    "del a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_device = 2\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "torch.cuda.set_device(gpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEF_SEND = '<SEND>'\n",
    "DEF_START = '<START>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATSET_FILE = 'traindataset_cnn.tar.gz'\n",
    "TEST_DATSET_FILE = 'testdataset_cnn.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir='/home/p.zaydel/ProjectNeuralNets/coco_dataset/'\n",
    "imagesDirTrain = '{}train2017/train2017'.format(dataDir)\n",
    "imagesDirVal = '{}val2017/val2017'.format(dataDir)\n",
    "\n",
    "annTrainFile = '{}/annotations_trainval2017/annotations/captions_train2017.json'.format(dataDir)\n",
    "annValFile = '{}/annotations_trainval2017/annotations/captions_val2017.json'.format(dataDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_tensor = transforms.Compose([\n",
    "                                transforms.ToTensor(), \n",
    "                                transforms.Normalize(\n",
    "                                    mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])\n",
    "                                           ])\n",
    "transform_to224 = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                      transform_tensor\n",
    "                                     ])\n",
    "transform_to500 = transforms.Compose([ transforms.Resize((500, 500)),\n",
    "                                      transform_tensor\n",
    "                                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_text2words(text):\n",
    "    symbs_to_replace = ['.', ',', '/', '-', ':', '{', '}', '[', ']', ]\n",
    "    for smb in symbs_to_replace:\n",
    "        text = text.replace(smb, ' ')\n",
    "    \n",
    "    \n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    for idx in range(len(words)):\n",
    "        words[idx] = spell(words[idx])\n",
    "    \n",
    "    words = [DEF_START] + words + [DEF_SEND]\n",
    "    \n",
    "    return words\n",
    "\n",
    "# def anns2words(anns_list):\n",
    "#     texts = []\n",
    "#     for anns in anns_list:\n",
    "#         for ann in anns['anns']:\n",
    "#             words = split_text2words(ann)\n",
    "#             texts.append(words)\n",
    "            \n",
    "#     return texts\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "def train_word_to_vec_gensim(dataset, embed_size = 300):\n",
    "    Texts = list(dataset.anns.values())\n",
    "    model = Word2Vec(Texts, size = embed_size, workers = 7, min_count = 0)\n",
    "    return model\n",
    "\n",
    "def generate_vocab_dicts(dataset): \n",
    "    Texts = list(dataset.anns.values())\n",
    "    uniqwords = list(set([w for ann in Texts for w in ann]))\n",
    "    words2ids = dict(zip(uniqwords, range(len(uniqwords))) )\n",
    "    ids2words = dict(zip(range(len(uniqwords)), uniqwords ))\n",
    "    return words2ids, ids2words\n",
    "\n",
    "\n",
    "def wordslist2wordids(words, word2id, vector_length = None ):\n",
    "    if vector_length is None:\n",
    "        word_ids = [word2id[w] for w in words]\n",
    "        \n",
    "    else:\n",
    "        word_ids = []\n",
    "        for idx in range(vector_length):\n",
    "            if idx < len(words):\n",
    "                w = words[idx]\n",
    "            else:\n",
    "                w = end_word\n",
    "                \n",
    "            word_ids.append(word2id[w])\n",
    "        \n",
    "        if word_ids[-1] != word2id[DEF_SEND]:\n",
    "            word_ids[-1] = word2id[DEF_SEND]\n",
    "        \n",
    "    return torch.from_numpy(np.array(word_ids).astype(np.int))\n",
    "\n",
    "\n",
    "def sentence2wordids(sentence, word2id, vector_length = None):\n",
    "    \n",
    "    if vector_length is None:\n",
    "        words = split_text2words(sentence)\n",
    "        word_ids = [word2id[w] for w in words]\n",
    "        \n",
    "    else:\n",
    "        words = split_text2words(sentence)\n",
    "        word_ids = []\n",
    "        for idx in range(vector_length):\n",
    "            if idx < len(words):\n",
    "                w = words[idx]\n",
    "            else:\n",
    "                w = end_word\n",
    "                \n",
    "            word_ids.append(word2id[w])\n",
    "        \n",
    "        if word_ids[-1] != word2id[DEF_SEND]:\n",
    "            word_ids[-1] = word2id[DEF_SEND]\n",
    "        \n",
    "    return torch.from_numpy(np.array(word_ids).astype(np.int))         \n",
    "    \n",
    "    \n",
    "import numpy as np\n",
    "# calculates dimension of alexnet convolutions layers output \n",
    "def get_alexnet_features_dim(imsize):\n",
    "    adim = int(np.round( 3*0.01*imsize - 1))\n",
    "    return 1*256*adim*adim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prepared_dataset(dataset, filename, cnn_model = models.alexnet(pretrained=True).features):\n",
    "    \n",
    "    cnn_model = cnn_model.cuda(gpu_device)\n",
    "    \n",
    "    print(\"preparing images...\")\n",
    "    for idx in tqdm(range(len(dataset))):\n",
    "        sample = dataset[idx]\n",
    "        var = Variable(sample['image'].unsqueeze(0)).cuda(gpu_device)\n",
    "        dataset.images_cnn[idx] =  cnn_model(var).data.view(-1).cpu()\n",
    "        \n",
    "    print(\"preparing annotations...\")\n",
    "    dataset.text_transform = split_text2words\n",
    "    dataset.preload_anotations()\n",
    "    dataset.text_transform = None\n",
    "    \n",
    "    torch.save(dataset, filename)\n",
    "    print(\"Dataset saved in {}\".format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_anns(dataset, annids, max_len, prepare = None):\n",
    "    '''\n",
    "       dataset - MSCOCODataset\n",
    "       annids -  tensor or numpy array\n",
    "       max_len - maximum len of sentence. If None computes from dataset \n",
    "       prepare - None or function to prepare each word, returns 1-dim tensor\n",
    "       \n",
    "       return Pytorch Tensor [len(annids) x max_sentence_len x prepare(word).shape[0] ]\n",
    "    '''\n",
    "    result = []\n",
    "    \n",
    "    if prepare is None:\n",
    "        prepare = lambda w: word_embeding[w]\n",
    "    \n",
    "    for i in range(annids.shape[0]):\n",
    "        words = dataset.get_ann(annids[i])\n",
    "        ann_res = []\n",
    "        \n",
    "        for idx in range(max_len):\n",
    "            if idx < len(words):\n",
    "                w = words[idx]\n",
    "            else:\n",
    "                w = DEF_SEND\n",
    "                \n",
    "            ann_res.append(prepare(w))\n",
    "        ann_res = torch.from_numpy(np.array(ann_res)).float()\n",
    "        result.append(ann_res)\n",
    "        \n",
    "    return torch.stack(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train dataset...\n",
      "train dataset loaded!\n",
      "loading test dataset...\n",
      "test dataset loaded!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(TRAIN_DATSET_FILE):\n",
    "    print(\"loading train dataset...\")\n",
    "    trainDataset = torch.load(TRAIN_DATSET_FILE)\n",
    "    print('train dataset loaded!')\n",
    "else:\n",
    "    trainDataset = MSCOCODataset(annTrainFile,imagesDirTrain, transform = transform_to224, mode='pic2rand')\n",
    "    save_prepared_dataset(trainDataset, TRAIN_DATSET_FILE, cnn_model)\n",
    "    \n",
    "if os.path.exists(TEST_DATSET_FILE):\n",
    "    print(\"loading test dataset...\")\n",
    "    testDataset = torch.load(TEST_DATSET_FILE)\n",
    "    print('test dataset loaded!')\n",
    "else:\n",
    "    testDataset = MSCOCODataset(annValFile,imagesDirVal, transform = transform_to224, mode='pic2rand')\n",
    "    save_prepared_dataset(testDataset, TEST_DATSET_FILE, cnn_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary......\n",
      "saving dictionary\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Creating dictionary......\")\n",
    "if os.path.exists('dictionaries_2.tar.gz'):\n",
    "    print(\"loading dictionary\")\n",
    "    dic_state = torch.load('dictionaries.tar.gz')\n",
    "    words2ids = dic_state['words2ids']\n",
    "    ids2words = dic_state['ids2words']\n",
    "    print(\"dictionary loaded\")\n",
    "else:\n",
    "    words2ids, ids2words  = generate_vocab_dicts(trainDataset)\n",
    "    print(\"saving dictionary\")\n",
    "    torch.save({'words2ids': words2ids, 'ids2words': ids2words }, 'dictionaries.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading words embedding\n",
      "words embedding loaded\n"
     ]
    }
   ],
   "source": [
    "# MY WORD EMBEDDINGS\n",
    "import os\n",
    "\n",
    "WORD_EMBED_FILE = 'word_embeding_6.tar.gz'\n",
    "if os.path.exists(WORD_EMBED_FILE):\n",
    "    print(\"loading words embedding\")\n",
    "    word_embeding = torch.load(WORD_EMBED_FILE)\n",
    "    print(\"words embedding loaded\")\n",
    "else:\n",
    "    print(\"creating words embedding......\")\n",
    "    word_embeding = train_word_to_vec_gensim(trainDataset, embed_size = 300)\n",
    "    print(\"saving words embedding\")\n",
    "    torch.save(word_embeding, WORD_EMBED_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTM_W2V_Net_Cnn_Preload(nn.Module):\n",
    "\n",
    "    def __init__(self,  image_size, image_features_size, word_embedding, words2ids, ids2words,\n",
    "                 lstm_hidden_size = 2000,\n",
    "                 word_embedding_size = 500, \n",
    "                 cnn = models.alexnet(pretrained=True).features,\n",
    "                 start_symbol = DEF_START,\n",
    "                 end_symbol = DEF_SEND\n",
    "              #   cnn_comp_features = lambda cnn, x: cnn.features(x),\n",
    "              #   max_sentence_len = 20,\n",
    "              #   sentence_end_embed = None,\n",
    "             #  sentence_end_symbol = '.'\n",
    "                  ):\n",
    "        \"\"\"Init NN\n",
    "            image_size - size of input image.\n",
    "            lstm_hidden_size - size of cnn features output\n",
    "            image_features_size - size of image features vector\n",
    "            word_embedding - pretrained word embedding model\n",
    "            words2ids - dictionary word -> id\n",
    "            ids2words - dictionary id -> word\n",
    "            cnn - pretrained cnn net (alexnet, vgg and other)\n",
    "            start_symbol - symbol starting sequence\n",
    "            end_symbol - symbol ending sequence\n",
    "        \"\"\"\n",
    "        \n",
    "        super(LSTM_W2V_Net_Cnn_Preload, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.image_features_size = image_features_size\n",
    "        #self.cnn = cnn\n",
    "     #   self.cnn_comp_features = cnn_comp_features\n",
    "        \n",
    "        self.vocab_size = len(words2ids)\n",
    "        \n",
    "        self.word_embedding_size = word_embedding_size\n",
    "        self.word_embedding = word_embedding\n",
    "        \n",
    "        self.words2ids = words2ids\n",
    "        self.ids2words = ids2words\n",
    "        \n",
    "        self.start_symbol = start_symbol\n",
    "        self.start_symbol_embed = torch.from_numpy(self.word_embedding[self.start_symbol])\n",
    "        \n",
    "        self.end_symbol = end_symbol\n",
    "        self.end_symbol_embed = torch.from_numpy(self.word_embedding[self.end_symbol])\n",
    "        \n",
    "#         self.sentence_end_symbol = sentence_end_symbol\n",
    "#         self.sentence_end_symbol_id = self.words2ids[self.sentence_end_symbol]\n",
    "        \n",
    "#         if sentence_end_embed is not None:\n",
    "#             self.sentence_end_embed = sentence_end_embed\n",
    "#         else:\n",
    "#             self.sentence_end_embed = word_embeding['.']\n",
    "        \n",
    "        #self.max_sentence_len = max_sentence_len\n",
    "        \n",
    "        \n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        \n",
    "        self.fc1 = nn.Sequential( nn.BatchNorm1d(self.image_features_size),\n",
    "                                  nn.Linear(self.image_features_size, int(self.image_features_size/2)),\n",
    "                                  nn.Dropout(0.001), \n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(int(self.image_features_size/2), int(self.image_features_size/4) ),\n",
    "                                  nn.Dropout(0.001),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(int(self.image_features_size/4), self.lstm_hidden_size),\n",
    "                                  nn.BatchNorm1d(self.lstm_hidden_size)\n",
    "                                ).cuda(gpu_device)\n",
    "        \n",
    "#         self.fc1 = nn.Sequential( nn.BatchNorm1d(self.image_features_size),\n",
    "#                                   nn.Linear(self.image_features_size, self.lstm_hidden_size),\n",
    "# #                                   nn.Dropout(0.001), \n",
    "# #                                   nn.ReLU(),\n",
    "# #                                   nn.Linear(int(self.image_features_size/4), self.lstm_hidden_size),\n",
    "#                                   nn.BatchNorm1d(self.lstm_hidden_size)\n",
    "#                                 ).cuda(gpu_device)\n",
    "        \n",
    "        self.fc2 = nn.Sequential(nn.Linear(self.lstm_hidden_size, self.vocab_size),\n",
    "                                  nn.LogSoftmax()\n",
    "                                ).cuda(gpu_device)\n",
    "        \n",
    "                               \n",
    "        self.lstm_cell = nn.LSTMCell(self.lstm_hidden_size + self.word_embedding_size, \n",
    "                                     self.lstm_hidden_size).cuda(gpu_device)\n",
    "        \n",
    "#         self.lstm = nn.LSTM(self.lstm_hidden_size , word_embedding_size)\n",
    "    \n",
    "        \n",
    "    \n",
    "#     def freeze_cnn(self):\n",
    "#         for param in self.cnn.parameters():\n",
    "#             param.requires_grad = False\n",
    "    \n",
    "#     def unfreeze_cnn(self):\n",
    "#         for param in self.cnn.parameters():\n",
    "#             param.requires_grad = True\n",
    "\n",
    "    def set_mode(self, mode):\n",
    "        if mode == 'train':\n",
    "            for layer in self.fc1:\n",
    "                layer.training = True\n",
    "                \n",
    "            for layer in self.fc2:\n",
    "                layer.training = True\n",
    "        elif mode == 'test':\n",
    "            for layer in self.fc1:\n",
    "                layer.training = False\n",
    "                \n",
    "            for layer in self.fc2:\n",
    "                layer.training = False\n",
    "            \n",
    "    def ids_to_embed(self, word_ids):\n",
    "        result = []\n",
    "        \n",
    "        for i in range(word_ids.shape[0]):\n",
    "            w = self.ids2words[word_ids[i].data[0]]\n",
    "            \n",
    "            emb = torch.from_numpy(self.word_embedding[w]).float()\n",
    "            result.append(emb)\n",
    "            \n",
    "        return torch.stack(result)\n",
    "        \n",
    "            \n",
    "            \n",
    "    def forward(self, X, max_sentence_len):\n",
    "        batch_size = X.shape[0]\n",
    "        \n",
    "        X = X.cuda(gpu_device)\n",
    "        \n",
    "        #X = self.cnn(X)\n",
    "        X = X.view(batch_size, self.image_features_size)\n",
    "        X = self.fc1(X)\n",
    "        \n",
    "        \n",
    "        X = X.cuda(gpu_device)\n",
    "        \n",
    "        # prevWord = START_SYMBOL\n",
    "        prevWord = Variable(self.start_symbol_embed.repeat(batch_size, 1), requires_grad=True).cuda(gpu_device)\n",
    "\n",
    "#         print('X', X.shape)\n",
    "#        print('pW', prevWord.shape)\n",
    "        lstm_input = torch.cat([X, prevWord], dim = 1).cuda(gpu_device)\n",
    "        \n",
    "        result = []\n",
    "        \n",
    "        h_t = Variable(torch.zeros(batch_size, self.lstm_hidden_size), requires_grad=False).cuda(gpu_device)\n",
    "        c_t = Variable(torch.zeros(batch_size, self.lstm_hidden_size), requires_grad=False).cuda(gpu_device)\n",
    "        \n",
    "#         print(lstm_input.shape)\n",
    "#         print(h_t.shape)\n",
    "#         print(c_t.shape)\n",
    "        \n",
    "        for idx in range(max_sentence_len):\n",
    "            h_t, c_t = self.lstm_cell.forward(lstm_input, (h_t, c_t))\n",
    "            probs = self.fc2.forward(h_t)\n",
    "            \n",
    "            top_word_ids = probs.max(1)[1]\n",
    "            embeds = self.ids_to_embed(top_word_ids)\n",
    "            embeds = Variable(embeds.cuda(gpu_device))\n",
    "            \n",
    "            \n",
    "#             return embeds\n",
    "            \n",
    "#             print(prevWord)\n",
    "            \n",
    "#             print(X.shape)\n",
    "#             print(embeds.shape)\n",
    "            \n",
    "#             print(embeds)\n",
    "            \n",
    "            lstm_input = torch.cat([X, embeds], dim = 1).cuda(gpu_device)\n",
    "#             print(probs.shape)\n",
    "            result.append(probs)\n",
    "            \n",
    "        #return result\n",
    "        result = torch.stack(result, dim = 1)\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/p.zaydel/conda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `layer1_size` (Attribute will be removed in 4.0.0, use self.trainables.layer1_size instead).\n",
      "  \n",
      "/home/p.zaydel/conda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/p.zaydel/conda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "image_size = 224\n",
    "image_features_size = get_alexnet_features_dim(image_size)\n",
    "\n",
    "lstmnet = LSTM_W2V_Net_Cnn_Preload(image_size, image_features_size, \n",
    "                       word_embeding, words2ids, ids2words, \n",
    "                       word_embedding_size = word_embeding.layer1_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"START NN TEST\")\n",
    "\n",
    "# trainDataLoader_2 = DataLoader(trainDataset, batch_size = 2, shuffle=True)\n",
    "# for sample in trainDataLoader_2:\n",
    "#     break\n",
    "\n",
    "# loss = nn.NLLLoss()\n",
    "# optimizer = torch.optim.Adam(lstmnet.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# optimizer.zero_grad()\n",
    "\n",
    "# X = sample['image']\n",
    "\n",
    "# X = Variable(X)\n",
    "\n",
    "# ann_ids = sample['anns']\n",
    "\n",
    "# batch_size = X.shape[0]\n",
    "# max_len = sample['ann_len'].max()\n",
    "\n",
    "# y = load_anns(trainDataset, ann_ids, max_len, prepare=lambda w: words2ids[w])\n",
    "# y = Variable(y.long())\n",
    "\n",
    "# pred = lstmnet.forward(X, max_len)\n",
    "\n",
    "# pred = pred.cpu()\n",
    "# y = y.cpu()\n",
    "\n",
    "# pred_b = pred.view(pred.shape[0]*pred.shape[1], pred.shape[2])\n",
    "\n",
    "# loss_batch = loss(pred_b, y.view(-1))\n",
    "\n",
    "# loss_batch.backward()\n",
    "# optimizer.step()\n",
    "\n",
    "# print(loss_batch)\n",
    "# print(\"TEST SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = trainDataset[0]\n",
    "# X = sample['image']\n",
    "# X = Variable(X).unsqueeze(0)\n",
    "# lstmnet.set_mode('test')\n",
    "# pred = lstmnet.forward(X, max_len)\n",
    "# lstmnet.set_mode('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = nn.CrossEntropyLoss()\n",
    "# input =Variable(torch.randn(2, 3, 5), requires_grad=True)\n",
    "# target = Variable(torch.LongTensor(2,3).random_(5))\n",
    "\n",
    "# target_onehot = torch.zeros(target.shape[0], target.shape[1],  5)\n",
    "# for i in range(target.shape[0]):\n",
    "#     target_onehot[i].scatter_(1, target.data[i].unsqueeze(1) , 1)\n",
    "\n",
    "# target_onehot = Variable(target_onehot)\n",
    "# target_onehot = target_onehot.view(target_onehot.shape[0]*target_onehot.shape[1], target_onehot.shape[2])\n",
    "# input = input.view(input.shape[0]*input.shape[1], input.shape[2])\n",
    "\n",
    "# loss(input, target.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint_1.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'best_'+filename)\n",
    "        \n",
    "def open_checkpoint(is_best = False, filename='checkpoint_1.pth.tar'):\n",
    "    if is_best:\n",
    "        filename = 'best_'+filename\n",
    "        \n",
    "    checkpoint = torch.load(filename)\n",
    "    return checkpoint\n",
    "#     best_prec1 = checkpoint['best_prec1']\n",
    "#     model.load_state_dict(checkpoint['state_dict'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-18bf08814031>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SAMPLE_ID =3455\n",
    "def test_nn_on_image(network, sample_idx = TEST_SAMPLE_ID):\n",
    "    network.set_mode('test')\n",
    "    sample = trainDataset[sample_idx]\n",
    "    ann_id = sample['anns']\n",
    "    ann = trainDataset.get_ann(ann_id)\n",
    "    im_id = sample['imid']\n",
    "    X = Variable(sample['image']).unsqueeze(0)\n",
    "    max_len = sample['ann_len']\n",
    "    \n",
    "    pred = lstmnet.forward(X, max_len)\n",
    "    wids = pred[0].max(1)[1]\n",
    "    \n",
    "    result = []\n",
    "    for i in range(wids.shape[0]):\n",
    "        wid = wids[i].data[0]\n",
    "        word = ids2words[wid]\n",
    "        result.append(word)\n",
    "        \n",
    "    network.set_mode('train')\n",
    "    \n",
    "    return {'res': result, 'ann_id': ann_id, 'imid': im_id, 'ann': ann, 'max_len': max_len}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TRAIN_LOG_FILE = \"train_log_1.txt\"\n",
    "TRAIN_PLT_FILE = 'train_plt.png'\n",
    "\n",
    "def train(network, train_dataloader, test_dataloader,\n",
    "          epochs,  loss = nn.NLLLoss(), optim=torch.optim.Adam ):\n",
    "    \n",
    "    print(\"TRAIN STARTED!\")\n",
    "    log_file = open(TRAIN_LOG_FILE,'w') \n",
    "#     if unfreeze_cnn_epoch is None:\n",
    "#         unfreeze_cnn_epoch = int(0.75 * epochs)\n",
    "    \n",
    "    train_loss_epochs = []\n",
    "    test_loss_epochs = []\n",
    "    optimizer = optim(network.parameters(), lr=0.001)\n",
    "    best_test_score = 10**6\n",
    "    \n",
    "    sheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    \n",
    "#     network.freeze_cnn()\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(epochs):\n",
    "            sheduler.step()\n",
    "\n",
    "            losses = []\n",
    "            accuracies = []\n",
    "            for sample in tqdm(train_dataloader):\n",
    "                   \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                X = sample['image']\n",
    "\n",
    "                X = Variable(X)\n",
    "\n",
    "                ann_ids = sample['anns']\n",
    "\n",
    "                batch_size = X.shape[0]\n",
    "                max_len = sample['ann_len'].max()\n",
    "\n",
    "                y = load_anns(trainDataset, ann_ids, max_len, prepare=lambda w: words2ids[w])\n",
    "                y = Variable(y.long())\n",
    "\n",
    "                pred = lstmnet.forward(X, max_len)\n",
    "\n",
    "                pred = pred.cpu()\n",
    "                y = y.cpu()\n",
    "\n",
    "                pred_b = pred.view(pred.shape[0]*pred.shape[1], pred.shape[2])\n",
    "\n",
    "                loss_batch = loss(pred_b, y.view(-1))\n",
    "                losses.append(loss_batch)\n",
    "\n",
    "                loss_batch.backward()\n",
    "                optimizer.step()\n",
    "  \n",
    "            train_loss_epochs.append(np.mean(losses))\n",
    "            losses = []\n",
    "        \n",
    "            for sample in test_dataloader:\n",
    "                X = sample['image']\n",
    "                X = Variable(X)\n",
    "                ann_ids = sample['anns']\n",
    "\n",
    "                batch_size = X.shape[0]\n",
    "                max_len = sample['ann_len'].max()\n",
    "\n",
    "                y = load_anns(trainDataset, ann_ids, max_len, prepare=lambda w: words2ids[w])\n",
    "                y = Variable(y.long())\n",
    "\n",
    "                pred = lstmnet.forward(X, max_len)\n",
    "\n",
    "                pred = pred.cpu()\n",
    "                y = y.cpu()\n",
    "\n",
    "                pred_b = pred.view(pred.shape[0]*pred.shape[1], pred.shape[2])\n",
    "\n",
    "                loss_batch = loss(pred_b, y.view(-1))\n",
    "                losses.append(loss_batch)\n",
    "                \n",
    "                \n",
    "            test_loss_epochs.append(np.mean(losses))\n",
    "            \n",
    "            image_test = test_nn_on_image(network)\n",
    "            \n",
    "            log_file.write(\"Epoch:{}\".format(epoch + 1))\n",
    "            log_file.write(\"Mean Test Loss:{}\".format(np.mean(losses)))\n",
    "            log_file.write(\"Test on image:\\n {}\".format(image_test))\n",
    "            \n",
    "            \n",
    "            is_best = test_loss_epochs[-1] < best_test_score\n",
    "            best_test_score = min(test_loss_epochs[-1], best_test_score)\n",
    "            save_checkpoint({\n",
    "                            'epoch': epoch + 1,\n",
    "                            'state_dict': network.state_dict(),\n",
    "                            'best_test_score': best_test_score,\n",
    "                            'optimizer' : optimizer.state_dict(),\n",
    "                            }, is_best)\n",
    "                \n",
    "            \n",
    "            sys.stdout.write('\\rEpoch {0}... (Train/Test) Loss: {1:.3f}/{2:.3f}'.format(\n",
    "                        epoch, train_loss_epochs[-1], test_loss_epochs[-1]))\n",
    "    except KeyboardInterrupt:\n",
    "        close(log_file)\n",
    "        pass\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(train_loss_epochs[1:], label='Train')\n",
    "    plt.plot(test_loss_epochs[1:], label='Test')\n",
    "    plt.xlabel('Epochs', fontsize=16)\n",
    "    plt.ylabel('Loss', fontsize=16)\n",
    "    plt.legend(loc=0, fontsize=16)\n",
    "    plt.grid('on')\n",
    "    plt.savefig(TRAIN_PLT_FILE)\n",
    "    \n",
    "    close(log_file)\n",
    "    \n",
    "    print(\"TRAIN ENDED!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataLoader = DataLoader(trainDataset, batch_size = 64, shuffle=True)\n",
    "testDataLoader = DataLoader(testDataset, batch_size = 64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1849 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN STARTED!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/p.zaydel/conda3/lib/python3.6/site-packages/torch/nn/modules/container.py:67: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n",
      "/home/p.zaydel/conda3/lib/python3.6/site-packages/ipykernel_launcher.py:119: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  1%|          | 16/1849 [00:09<18:40,  1.64it/s]/home/p.zaydel/conda3/lib/python3.6/site-packages/tqdm/_monitor.py:89: TqdmSynchronisationWarning: Set changed size during iteration (see https://github.com/tqdm/tqdm/issues/481)\n",
      "  TqdmSynchronisationWarning)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1518243271935/work/torch/lib/THC/generic/THCStorage.cu:58",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-10750724f0a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstmnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-54f856958657>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(network, train_dataloader, test_dataloader, epochs, loss, optim)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0mloss_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1518243271935/work/torch/lib/THC/generic/THCStorage.cu:58"
     ]
    }
   ],
   "source": [
    "train(lstmnet, trainDataLoader, testDataLoader, 20, loss = nn.NLLLoss() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
